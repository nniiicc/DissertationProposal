#A controlled vocabulary 

*This document contains general notes and things I wanted to formally define for myself so that I could continue to reference concepts consistently. These are my definitions they are by no means official.*


#####Cultures

**Epistemic culture**        
Those amalgams of arrangements
and mechanisms-bonded through affinity, necessity, and historical coincidence-
which, in a given field, make up how we know what we
know. Epistemic cultures are cultures that create and warrant knowledge,
and the premier knowledge institution throughout the world is,
still, science. (Karen Knorr-Cetina 1999, p. 1)     

**Evidential culture**        
There are two kinds of evidential cultures -open and closed - and they can be compared along three dimensions: evidential collectivism vs evidential individualism, evidential signifcance, and evidential thresholds.         

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*Evidential individualists vs. collectivists*        
Individualists believe it is job of individual to take responsibility for the validity and meaning of scientific results. Collectivists believe it is job of community to asses results at an early stage. In both cases, the acceptance of a fact is for the community to agree - but the key difference is that indivdualists see rejection as a professional failure, where objectionists see discussions (rejection, clarification, acceptance) to be a normal part of scientific process. This can also has implications for how community governs itself - collectivists place onus of correctness on community (such as drivers and safety in Italy) whereas the individualists place onus of correctness on individual - and violation of that correctness are met with sanctions. ( Slight variation on guilt or shame cultures, yes?)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*Evidential significance*        
The more important the claim, the greater the risk of engendering opposition, and the longer the change of inference, the more ways there are to be wrong. High evidential significance entails 'interpretive risk' while low evidential significance involves low interpretive risk. The higher the evidential significance, the longer the chance of inference, and the more important the findings. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*Evidential thresholds*        
Regardless of other factors, a scientific culture might be more or less risk averse in terms of the level of certainty in the data that is taken to merit announcement or publication. This can also be though of as a risk in statistical sense - how many standard deviations are acceptable to merit 'discovery' - in most sciences 2-3, in HEP 7 -8. Low evidential thresholds = high statistical risk, and vice versa. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Closed evidential cultures**
low collectivism, high significance and high threshold

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Open evidential cultures**
high collectivism, low significance, and low threshold

#####Sciences

**Normal Science**        
“...research firmly based on one or more past scientific achievements, achievements that some particular scientific community acknowledges as supplying the foundations for its further practice” (Kuhn 1970, p. 10) Facts and findings are not under dispute, value is seen through the broad applications and repeated findings. Quality and legitimacy of the epoch is not strongly questioned (or questioned at the periphery).

Also worth noting, that paradigms then are the periods of normal science, in which practitioners are engrossed and enrolled in a normal research agenda - the incredible thing about a paradign of normal science is that it rests upon a bedrock of well founded, highly significant (undisputed and unquestioned) findings, yet leaves a number of theoretical, and practical issues open for investigation. The action happens at the periphery. 

**Post-normal Science**
In post-normal sciences, the actions is at the very crux of the paradigm. In it's simplest form we can say that post-normal science has the following qualities ...facts are uncertain, values in dispute, stakes high, and decisions urgent. 

More abstractly, "…science is no longer imagined
as delivering truth, and it receives a new organizing principle, that of quality. This is dynamic,systemic and pragmatic, and therefore requires
a new methodology and social organization
of work (Funtowicz and Ravetz, 1993). The principle
of quality enables us to manage the irreducible
uncertainties and ethical complexities that
are central to the resolution of issues in post-normal
science. It entails the democratization of
knowledge by an extension of the peer-community
for quality assurance…post-normal science encompasses
the multiplicity of legitimate perspectives
and commitments, and provides new norms of
evidence and discourse." (Funtowitz and Ravetz, 1994)

**Linked Science**        
Represents a post-normal, open evidential culture. Its ambition is to endow traditional knowledge products with actionable, executable properties - linked open data, commons licensing, remotely accessible infrastructures, and reusable / shareable products - all the way down. 

#####Stuff

**Technical Artifacts**        
Technical artifacts include all the common objects of everyday life such as toilets, paper clips, tablets and dog collars. They are intentionally produced things. This is an essential part of being a technical artifact. For example, a physical object that accidentally carries out arithmetic is not by itself a calculator. This teleological aspect distinguishes them from other physical objects, and has led philosophers to argue that technical artifacts have a dual nature fixed by two sets of properties  
      
- Functional properties: say what the artifact does. For example, a kettle is for boiling water and a car is for transportation      
  
- Structural properties: pertain to its physical makeup. They include its weight, color, size, shape, its chemical constitution etc. In particular, we might insist that our car is red and has white seats. (Turner, 2013)

Less discussed, and more controversial is the idea that technical artifacts have emergent properties "… emergent entities (properties or substances) ‘arise’ out of more fundamental entities and yet are ‘novel’ or ‘irreducible’ with respect to them. (For example, it is sometimes said that consciousness is an emergent property of the brain.)" (O'connor and Wong, 2012)

For the sake of this dissertation, value and impact are treated as emergent properties of a technical artifact. 

**Technological Artifacts**

**Epistemic Things**

Why I am avoiding an extended talk of epistemology ---> "I think it is fair to say that the metaphysical grandness of ‘epistemology’ continued to be hinted at, even while epistemologies became fragmented and epistemology was divested of its transcendent unity, to the point that the word ‘epistemology’ often seemed to be a fancy way to speak of a specific belief or
belief system." <--- Ontography by LAw

####Value
Most generally, value theory describe how, why and to what extent an actor values some thing (object, idea, moral, other person, right, etc. ) - it is important to discern between value and goodness - where goodness was first assumed to be the moral and ethical quality of the thing being valued, through time good has come to have a less abstract and more economic, social and psychological dimension- such as the value of a artifact.  

- Moral good: conduct along ethical and moral dimensions
- Natural good: 

####Method

**Interactional Expertise**
Collins proposes three types of expertise - contributory (can apply for a job in the domain), no expertise (sociologist studying a domain w/ no training) and interactional expertise (you get jokes, can read the papers). The turing test is a good heuristic for expertise - and the interactional expert should be able to pass it. 


#Modelling Primer 
**Model Types** GCM – Global Circulation Model        EMIC - Earth System Models of Intermediate Complexity        
ESM- Earth System Model        **Model products**The services, datasets, and captured workflows that are used to generate model output, including the output files themselves. Models might be differentiated in the following ways: 1.	Programming language they are written in (C,  Fortran, Perl, Python etc)2.	Computational grid (space between observation points, and way grids are configured3.	Time stepping skips (Intervals such as days, years, hours)4.	Use of different input / output file formatsThe above differences between models pose complications for the coupling of models. **Model coupling**        Bringing together two separate models for a more comprehensive ‘system’ view of a phenomena. (need expansion). A coupled model then is made up of two independently developed models that interact by exchanging information on a few key variables.
**Model run**        A completed time step in a model simulation. This generally produces an intended output, and is then captured and stored for analysis. **Ensemble**         Ensemble experiments consist of multiple climate-model runs that are used in studying and quantifying the uncertainty in climate-model output. In the most generic of senses, an ensemble is setting slightly different initial conditions for modeling future states, and then studying how the different initial conditions effects that future state in terms of the model outputs. As explained by another climate modeling primer; Ensemble simulation. To test the role of initial conditions and to gain an estimate of uncertainty in climate predictions, climate models can be integrated repetitively from slightly different initial conditions, providing a number of ‘different’ simula- tions covering the same period in the future. Such a climate ensemble provides an estimate of the inherent unpredictability of the model and, perhaps, a measure of natural variability**Community modeling**        
A community model is developed and distributed in an open source or ‘free software’ environment, and attempts to leverage the use and users of the model in improving, debugging and distributing the model.  **Forcing**        A change in an internal or external factor which affects the climate.**Integration**        
Fundamental problem of how to bring together a diverse collection of existing resources (models and databases) to solve new problems with the least effort. (Peckham, 2012)**Interface** – (this is very importantly different than is used in other settings, like HCI)          
A named set of fucntions (called methods) with specific names, arguments and returned data types that are used to retrieve data from another model or database. However,Plug and play- This is a method of software development that modularizes different functionalities of a model into components, so that components can be ‘wrapped’ and ‘plugged’ into a new model. **Tuning**        Tuning refers to the tendency to adjust coefficients or reconstruct equations to achieve a better simulation result. Tuning may appear as very close to parameterizing, but whereas tuning is part of verifying a simulation model in use, the development of parameterizations is part of building the simulation model (Sundberg 2008 p 7-8The problem with tuning is that it artificially prevents a model from producing a bad result. . . . It might be argued that tuning is justified in the service of numerical weather prediction, because a good forecast is an end in itself, regardless of how it has been obtained. The trouble with this “end justifies the means” argument is that in the long run better scientific understanding is the key to making better forecasts. (Randall and Wielicki 1997, 404)High performance computing (HPC) : **Resolution**(The smaller the distance between the grid points and the smaller the time step, the higher the resolution of the simulation model. Sundberg, 2008)Parameterization: Most often, parametrization is a mathematical process involving the identification of a complete set of effective coordinates or degrees of freedom of the system, process or model, without regard to their utility in some design. (Wikipedia) What I understand of it right now, is somewhat boundary making. (**USERS**Model Construction: Winsberg (1999, 2001, 2003) has described simulation model construction as a transformation of mathematical models (which are based on theoretical models) into algorithms that are turned into computer code. (Sundberg 2008, p. 3)Reanalysis: Past observational data are used with advanced assimilation techniques and forecast models to produce retrospective datasets typically on a global grid at approximately 10x10 spatial resolution and at 6-hour intervals. The earliest reanalysis starting date is 1948 and frequently other reanalyses begin in 1979, which coincides with the advent of good satellite data coverage. (Jacobs and Worley, 2010) **DATA & MODELS**In Use:  Basic steps around using data for modeling1. Discover or identify data2. Assess the relevance, uncertainty and quality of the data and their fitness for application (use)3. Acquire data for process and analysis.4. Prepare data for processing. (Subsetting, gridding, interpolating, subsampling etc.)Crude workflow of a model user: 1. Locate or discover database / respopsitory holding data of interest- interest is based on requirements of study site, and coverage of time period. 2. Determine how to obtain data from that source3. Convert or manipulate obtained data into form that is required by modelWays that models use data (Peckham et al 2012)1. To set initial values of model state variables2. To set boundary conditions ( coastline position, bathymetry)3. to for or drive (often called steering) the model (e.g. rainfall, air temperature, tides)4. To compare predictions to observations (assessing the model) 5. To calibrate, tune, or train the model. **Data handling (for models)**  
The  discovery assessment, acquisition and preparation of data for use in a model. **Data Discovery**the basic, early step in modeling where a scientist identifies what data exists in his/her time series, gride, phenomena that can be used in the model. **Data Assessment**In the earth sciences, even heavily instrumented data collection is prone to error, and the mere fact that it is observational data means that there will be uncertainty in the collection process.  Scientists asses data based on indications of quality, such as who gathered it, how it was gathered, where and when it was gathered, the documentation and metadata that provide context about it, the reliability of the institution or person that gathered it and even what format or file structure it is stored in (which effects access). Parsons adds that (2012) “In addition to data quality considerations, modelers have additional assessment criteria, such as whether the data are at an appropriate scale, have the necessary temporal and spatial coverage, etc.” **Data acquisition**  Like discovery, acquisition is prone to many technical and social barriers. Because of both size and the complexity / heterogeneity of many  data sources needed for modeling there is an increasing challenge in physically transferring data from one repository / cloud storage service to another in order to complete further analysis. Acquisition then is both the negotiation of data transfer, and/ or the physical act of ‘getting’ the data. When access to data can facilitate remote analysis and manipulation, then  transfer loads and data preparation requirements (for ingest) can and will decline (Parsons, 2012). Peckham et al (2012) note that  obtaining data is often the most labor-intensive step for generating new model runs. **Data preparation**  Because of the diversity of questions asked by modelers, often the data that is available to both access and use will need to be reformatted, subset, normalized, or transformed in some way to be used in a model. These curatorial tasks are essentially what makes the fit between data and a model possible- and subjective decisions about the worth of a data source are often tightly coupled with the amount of perparation work necessary for the source to be used by a modeler. Parsons (2012) explains further “Some of these issues are well understood and can be addressed through tools and techniques such as automated subsetting and reformatting, but project or discipline-specific models can use highly specialized data structures, resolutions, and time/space domains (e.g. hunting tags vs. polar orbiting satellite data). Furthermore, data preparation requirements will vary depending on where in the modeling protocol data are being used.”**3 Data Types important to modeling:**  1.	boundary or initialization data,  2.	model algorithm test or benchmark data, and  3.	integrated datasets for model validation of coupled systems.  Dynamic modelA physical, mathematical description of changes in important geological variables, such as dissolution of mineral or variations in thickness of river deposits. The translation is what should be emphasized. It is what science contributes, is the translation of natural phenomena to a mathematical formula. "A system is a big black box of which we cant unlock the locks, And all we can find out about is what goes in and what comes out" Kenneth BouldingThe process of modeling is to provide illuminations - that is to give insight into the connections and processes of a system that otherwise seems like a big black box.  -- so for the authors of this book - the earths systems may EACh be a black box, but a well-formulated model is the key that lets you unlock the locks and peer inside. *Mathematical Modeling: Primer from NCAR lecture**Model direction*Forward models - project the final (or proposed boundary) state of a system   Inverse models - which take a solution and attempt to determine the initial and boundary conditions that gave rise to it *Model types*Conceptual - physical models and chemical experimentsStochastic - structure imitating Deterministic - process imitating Earth Science models tend to be of a deterministic, forward model. These types are physical-mathematical - descriptions of temporal or spatial changes in important geological variables, as derived form accepted laws, theories and empirical relationships. Models are "devices that mirror nature by embodying empirical knowledge in forms that permit (quantitative inferences to derived from them." (Dutton 1987)Laws like:-conservation laws, laws of hydraulics, first-rder rate laws from material fluxes of IC'sIs modelling inductive? 
>"We call modeling an art because one must know what one wants out of a model and how to get it. Properly constructed, a model will rationalize the information coming to our sense, tells us what the most important data are, and tell us what data will best test our notion of how nature works as it is embodied in the model.-- on the other hand a bad model is either 'too complex / simple or uneconomical'The logic of dynamic models is about dispelling intuition:If my premises are true:The math is true:: Solutions must be true. Advantages
- shrinks geological time- permit formulation of hypothesis for testing complex outcomes, nonlinear couplings and distant feedbacks. Page three contains a simple example for referencing in future talks -Many limitations to dynamic models - see Pilkey 2007 for more discussion - However, don't lose sight of the ambition of modeling earth science phenomena "it is not possible to maximize generality, realism and precision” (Dutton 1982) We assume, largely that a fruitful way to describe the earth is a series of mathematical equations.  > "Attempting to extract dynamics at higher levels form comprehensive modeling of everything going at a lower level is … is like analyzing the creation of La Boheme as a neurochemistry problem (Paola 2000).  [Data Assimilation](http://robinson.seas.harvard.edu/PAPERS/red_report_62.html)###Basic definitions*Independent variables*
(those that are constant or constant rates) like time and space*Dependent variables*
those that vary given the independent variable. *Derivatives*
The rate of change of one variable with respect to another is called a derivative, represented by equation dV / dt . Equations that express a relationship a relationship among these variables and their derivatives are called differential equations *Verification*
 The process of determining that model implementation accurately represents your conceptual decryption of the model and the solution to the model. So verification includes checking that the coding correctly implements the equations and models*Validation*Validation determines the degree to which model is an accurate representation of the world from the prospective of its intended use. ##Model Theory – What are Models A theory is a linguistic entity consisting of a set of sentences and models are non-linguistic entities in which the theory is satisfied.A model for Suppes is a set-theoretical structure consisting of a set of objects together with properties, relations, and functions defined over the set of objects.For Suppes –A theory is a set of axiomsA model is a set of objects satisfying the axiomsInstantial Models and Analogy (need a better understanding of this) These models will all be isomorphic, that is, there will be a one-to-one correspondence between the elements of the different models, more or less this means that models Are ANALOGIES Representational ModelsModels are not primarily providing a means for interpreting complex systems, but as tools for representing the world. (Giere says this is not their only function, but it is perhaps the central function of models used in empirical sciences) Models as representational – they have a resolution, which allows us to navigate, and understand a certain phenomena, the same way the resolution of a map allows us to get an orientation for our surroundings –but is only ever a partial representation of the place we are looking at. Borges story of the mapping of the world too realistically? To generalize: A map represents the region mapped in virtue of shared spatial similarities between the map and the region mapped. Here one object (a map) is used to represent another object (a geographic region). This notion is explicitly opposed to that of a statement representing a state of affairs.But there is no need to look for a general account of similarity between a model and what is modeled. Similarity is context dependent. In any particular context, what is said to be similar to what, in what ways, and to what degrees, can be specified.*Material Models*
Diaragrams: generally act like representational models. They represent ‘connections’ of things rather than actual placement. This is due in part to the spatial placement of the thing being modeled having a relative unimportance to the system or idea being modeled (think of an electrical circuit, the connections  (what connects to what) are much more import than where the connections are soldered onto a circuit board)Scale models: (car, house, or even solar systems) canonical example is that of James Watsons helical structure of DNA molecules- which was representational in virtue of thee dimensional spatial and structural similarities between the scale model and real DNA molecules. *Abstract Models*(4) Y= ax + b – this is abstract in the sense that it is purely mathematical, as opposed to what Giere calls abstract models, which replace variable with concrete real objects and their relations. So, if we want to model an automobiles intersection with a particular point, we could assign the variables yax and b to things like velocity, distance etc. and use the model to calculate a time of intersection (or really we can solve for many variables) “One may say that, in the model, this equation is true. What one cannot say is that the equation is true of the position of a real auto. No real auto can maintain a genuinely constant velocity in a perfectly straight line. The question, as always, is how similar the real situation is to the model of the situation.” By introducing margins of error, we can more precisely tie the real auto, with the auto in our model, and hence the idea that representation in science is to be understood solely in terms of the truth of statements. *Models as Hypotheses*Instead of rejecting the model because it doesn’t correspond directly with reality (a positivistic view) there is is use for statements like (4), such as saying that some particular real system is similar to the proposed model, to the degree specified by the various error terms. It is also possible to generalize statements like (4) to cover classes of systems. In this case, the error terms might be explicitly specified, or they may be place-holders for the specific error terms in the various individual hypotheses included in the generalization.*Mathematical Models*Much mathematical modeling proceeds in the absence of general principles to be used in constructing models. Rather, one has a number of different mathematical techniques useful for constructing models, such as differential equations and systems of linear equations. These are deployed as the situation requires.For example, in modeling the growth of organic populations, as in ecology, two sorts of models one may employ are exponential models and logistic models.**Models and Theories**Morgan and Morrison discuss models as a mediators, that models exist as autonomous agents operating between data and theory. While their discussion is deep, well constructed and compelling, I find it ultimately wrong, or at least wrong with respect to computational modeling in earth science. They do not address, ontologically what models are – nor theories. They do imply that as models progress they might become theories (how!) They also do not suggest that models are simplifcations, whereas theories are much less so. “…reasoning about the world is primarily reasoning with models. It is models almost all the way up.” “These are like horrific data producing machines – I could fill our mass storage (2pb) in a week with a CESM installation and wifi connection” Gary Strand, evil genius and software engineer at NCAR.**Models and Data**James Woodward : Theories explain phenomena – and Models explain dataOn this view, when testing the fit of a model with the world, one does not compare that model with data but with another model, a model of the data. Thus one reasons from a high level model not to predictions about data, but to predictions about a model of possible data. The actual data are processed in various ways so as to fit into a model of the data. It is this latter model, and not the data itself, that is used to judge the similarity between the higher level model and the world. In between, as Suppes already insisted, must be a model of the experiment. A version of Suppes’ hierarchy of models is shown in Figure 7. It is models almost all the way downInstantial vs RepresentationalOn the instantial view of models, there are direct relationships of reference and truth between linguistic expressions and objects, as pictured in Figure 8. This conception works fine for well-defined abstract objects. It does not work so well for physical objects, whose natures and relationships are not so well-defined. On a representational conception of models, language connects not directly with the world, but rather with a model, whose characteristics may be precisely defined. The connection with the world is then by way of similarity between a model and designated parts of the world, as shown in Figure 9.
